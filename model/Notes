4.15:
Read the TGN fake news detection again.

They are batch training, we can too.
We can adapt the tgn to a tgn-layer, and this layer encode with memory and node features, just as what you are doing now.

After each tgn-layer, we get the temporal embedding, put it into GCL and compute with TDN, then get prediction label.
It's difficult but reachable.

The batch size need to also be defined, don't worry about whether batch training will affect the 'continuous', it does but not serious.

4.18:
my_embedding_module, 取消neighbor_finder作为构造函数参数存在，neighbor_finder从外部生成，只在forward时候作为一个参数传入. (DONE)

my_embedding_module, 取消memory 作为构造函数参数存在，与neighbor_finder一致. (DONE)

能够取消是因为， neighbor_finder 和 memory 都是无梯度，只作为一个data structure 而存在。 (DONE)

# model可以生成，memory可以重制，但是我们不再需要negative samples。去掉这个功能 (DONE)

# 在模型处理每一个图的时候还要创建一个新的node features set 和 edge feature set 以保存所有的点，边数据。 (DONE)
Set init_memory as init_event，在处理一个新的event时候需要处理的不只是memory， 还有node 和 edge feature. (DONE)

#  他本来网络的 node_feat_shape, edge_feat_shape: (9228, 172) (157475, 172)


4.18：

? tgat forward 函数还有什么未完成？ 我把他们代码生成的数据保存下来，取一小部分数据集输入tgat， 查看是否能正常运行！
